import triton
import triton.language as tl
import torch

# -----------------------------------------------------------------------------
# 1. Triton Kernel å®ç° (å·²ä¿®å¤æŒ‡é’ˆè®¡ç®— Bug)
# -----------------------------------------------------------------------------
@triton.jit
def softmax_tlkernel(
    X,
    Y,
    stride_x,
    stride_y,
    N,
    BLOCK_SIZE: tl.constexpr
):
    # 1. è·å–å½“å‰å¤„ç†çš„è¡Œå·
    row_idx = tl.program_id(0)
    
    # 2. è®¡ç®—å½“å‰è¡Œçš„æŒ‡é’ˆä½ç½®
    x_row_ptr = X + row_idx * stride_x
    y_row_ptr = Y + row_idx * stride_y
    
    # 3. ç”Ÿæˆåˆ—åç§»é‡ [0, 1, ..., BLOCK_SIZE-1]
    offsets = tl.arange(0, BLOCK_SIZE)
    # 4. ç”Ÿæˆæ©ç ï¼Œé˜²æ­¢è¶Šç•Œ (å¤„ç† N ä¸æ˜¯ 2 çš„å¹‚æ¬¡çš„æƒ…å†µ)
    mask = offsets < N
    
    # 5. åŠ è½½æ•°æ®
    # BUG FIX: åŸä»£ç  x_ptr = x_row_ptr + mask æ˜¯é”™è¯¯çš„
    x_ptr = x_row_ptr + offsets
    
    # åŠ è½½è¾“å…¥è¡Œæ•°æ®ï¼Œè¶Šç•Œéƒ¨åˆ†å¡«å……è´Ÿæ— ç©·å¤§ï¼ˆä¸å½±å“ Maxï¼‰
    input_val = tl.load(x_ptr, mask=mask, other=-float('inf')).to(tl.float32)
    
    # 6. Online Softmax é€»è¾‘ (Safe Softmax)
    # æ‰¾åˆ°å½“å‰è¡Œçš„æœ€å¤§å€¼
    max_val = tl.max(input_val, axis=0)
    # å‡å»æœ€å¤§å€¼ï¼Œæ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–
    input_val = input_val - max_val
    
    # è®¡ç®—åˆ†å­ (exp)
    numerator = tl.exp(input_val)
    # è®¡ç®—åˆ†æ¯ (sum)
    denominator = tl.sum(numerator, axis=0)
    
    # 7. è®¡ç®—æœ€ç»ˆç»“æœ
    y = numerator / denominator
    
    # 8. å†™å›ç»“æœ
    y_ptrs = y_row_ptr + offsets
    tl.store(y_ptrs, y, mask=mask)

# -----------------------------------------------------------------------------
# 2. Python åŒ…è£…å‡½æ•°
# -----------------------------------------------------------------------------
def softmax(x):
    M, N = x.shape
    # Block Size å–å¤§äº N çš„æœ€å° 2 çš„å¹‚æ¬¡
    BLOCK_SIZE = triton.next_power_of_2(N)
    y = torch.empty_like(x)
    
    # æ¯ä¸ª Program å¤„ç†ä¸€è¡Œ
    grid = (M, )
    
    # è®¾ç½® num_warps ä»¥ä¼˜åŒ–å¤§ Block çš„æ€§èƒ½
    num_warps = 4
    if BLOCK_SIZE >= 2048:
        num_warps = 8
    if BLOCK_SIZE >= 4096:
        num_warps = 16

    softmax_tlkernel[grid](
        x, y,
        x.stride(0), y.stride(0),
        N,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=num_warps
    )
    return y

# -----------------------------------------------------------------------------
# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸ç†è®ºå¯¹æ¯”åˆ†æ
# -----------------------------------------------------------------------------
def benchmark_on_h100():
    # H100 SXM5 å‚æ•°
    GPU_NAME = torch.cuda.get_device_name(0)
    # å¦‚æœæ˜¯ H100 SXM5, ç†è®ºå¸¦å®½çº¦ 3350 GB/s (3.35 TB/s)
    # å¦‚æœæ˜¯ H100 PCIe, ç†è®ºå¸¦å®½çº¦ 2000 GB/s
    THEORETICAL_BW_GBPS = 2000.0 
    
    print(f"Running on GPU: {GPU_NAME}")
    print(f"Target Theoretical Bandwidth: {THEORETICAL_BW_GBPS} GB/s (H100 SXM5 estimate)")
    
    # æµ‹è¯•æ•°æ®è§„æ¨¡ (æ¨¡æ‹Ÿ Llama-3-70B çš„ Attention Score çŸ©é˜µ)
    # Batch * Head * Seq_Len * Seq_Len
    # å‡è®¾æˆ‘ä»¬æµ‹è¯•ä¸€è¡Œå·¨å¤§çš„æ•°æ®æˆ–è€…å¤šè¡Œæ•°æ®
    # è¿™é‡Œä¸ºäº†ç¨³å®šæµ‹é€Ÿï¼Œè®¾ç½® M ä¸ºè¾ƒçš„å¤§æ•°å€¼
    BATCH = 32 * 128  # Batch * Heads
    SEQ_LEN = 8192    # Sequence Length (N)
    
    print(f"\n--- Benchmarking Configuration ---")
    print(f"Shape: [{BATCH}, {SEQ_LEN}]")
    print(f"Data Type: FP16 (2 bytes)")
    
    # å‡†å¤‡æ•°æ®
    x = torch.randn(BATCH, SEQ_LEN, device='cuda', dtype=torch.float16)
    
    # 1. æ­£ç¡®æ€§éªŒè¯
    y_triton = softmax(x)
    y_torch = torch.softmax(x.float(), dim=1).half() # Torch softmax åœ¨ fp16 ä¸‹å¯èƒ½ä¸ç¨³å®šï¼Œè½¬ fp32 ç®—å®Œè½¬å›
    
    if torch.allclose(y_triton, y_torch, atol=1e-2, rtol=1e-2):
        print("âœ… Correctness Check Passed!")
    else:
        print("âŒ Correctness Check Failed!")
        print("Max Diff:", (y_triton - y_torch).abs().max().item())

    # 2. æµ‹é‡å®é™…è¿è¡Œæ—¶é—´ (ms)
    # triton.testing.do_bench ä¼šè‡ªåŠ¨å¤„ç† Warmup å’Œå¤šæ¬¡æµ‹é‡å–å¹³å‡
    ms = triton.testing.do_bench(lambda: softmax(x))
    
    # 3. è®¡ç®—å®é™…ååé‡ (GB/s)
    # Online Softmax è®¿å­˜é‡: Read X (2N) + Write Y (2N) = 4N Bytes per row
    # Total Bytes = 4 * M * N
    total_bytes = 4 * BATCH * SEQ_LEN
    actual_bw_gbps = (total_bytes * 1e-9) / (ms * 1e-3)
    
    # 4. è®¡ç®—ç†è®ºæ—¶é—´ (ms)
    # Time = Total Traffic / Bandwidth
    theoretical_ms = (total_bytes * 1e-9) / THEORETICAL_BW_GBPS * 1000
    
    # 5. æ‰“å°å¯¹æ¯”æŠ¥å‘Š
    print(f"\n--- Performance Results ---")
    print(f"Actual Runtime       : {ms:.4f} ms")
    print(f"Theoretical Runtime  : {theoretical_ms:.4f} ms (Based on {THEORETICAL_BW_GBPS} GB/s)")
    print(f"Actual Bandwidth     : {actual_bw_gbps:.2f} GB/s")
    print(f"Bandwidth Utilization: {actual_bw_gbps / THEORETICAL_BW_GBPS * 100:.2f}%")
    
    print(f"\n--- Analysis ---")
    if actual_bw_gbps / THEORETICAL_BW_GBPS > 0.75:
        print("ğŸš€ Excellent! The kernel is Memory Bound and highly efficient.")
    else:
        print("âš ï¸  Room for improvement. Consider tuning num_warps or checking memory coalescing.")

if __name__ == "__main__":
    torch.manual_seed(0)
    if torch.cuda.is_available():
        benchmark_on_h100()
    else:
        print("CUDA not available, cannot run benchmark.")