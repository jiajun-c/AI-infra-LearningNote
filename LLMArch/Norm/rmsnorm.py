import torch
import torch.nn as nn
class RMSNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6, elementwise_affine=True):
        super().__init__()
        if isinstance(normalized_shape, int):
            normalized_shape = (normalized_shape,)
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
        else:
            self.weight = None

    def forward(self, x: torch.Tensor):
        # ç¡®å®šå½’ä¸€åŒ–çš„ç»´åº¦ï¼šæœ€å len(normalized_shape) ä¸ªç»´åº¦
        norm_dims = tuple(range(-len(self.normalized_shape), 0))
        # è®¡ç®— RMSï¼šsqrt(mean(x^2) + eps)
        rms = torch.sqrt(x.pow(2).mean(dim=norm_dims, keepdim=True) + self.eps)
        # å½’ä¸€åŒ–
        x_normed = x / rms
        if self.elementwise_affine:
            x_normed = x_normed * self.weight

        return x_normed
    
class RMSNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6, elementwise_affine=True):
        super().__init__()
        if isinstance(normalized_shape, int):
            normalized_shape = (normalized_shape,)
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
        else:
            self.register_parameter('weight', None)

    def forward(self, x: torch.Tensor):
        # ç¡®å®šå½’ä¸€åŒ–çš„ç»´åº¦ï¼šæœ€å len(normalized_shape) ä¸ªç»´åº¦
        norm_dims = tuple(range(-len(self.normalized_shape), 0))
        rms = torch.sqrt(x.pow(2).mean(dim=norm_dims, keepdim=True) + self.eps)
        x_normed = x / rms
        if self.elementwise_affine:
            x_normed = x_normed * self.weight
        return x_normed

# ----------------------------
# æµ‹è¯•å‡½æ•°
# ----------------------------
def test_rmsnorm():
    torch.manual_seed(42)
    
    # æµ‹è¯•ç”¨ä¾‹ï¼š(input_shape, normalized_shape)
    test_cases = [
        ((2, 3), 3),
        ((4, 5, 6), 6),
        ((2, 8, 10, 12), (10, 12)),
        ((1, 768), 768),
    ]
    
    for input_shape, norm_shape in test_cases:
        print(f"\nTesting input shape {input_shape} with normalized_shape {norm_shape}")
        
        # åˆ›å»ºæ¨¡å—
        rmsnorm = RMSNorm(norm_shape, eps=1e-6, elementwise_affine=True)
        x = torch.randn(input_shape, requires_grad=True)
        
        # å‰å‘
        y = rmsnorm(x)
        
        # æ‰‹åŠ¨è®¡ç®—éªŒè¯
        if isinstance(norm_shape, int):
            norm_dims = (-1,)
        else:
            norm_dims = tuple(range(-len(norm_shape), 0))
        rms_manual = torch.sqrt(x.pow(2).mean(dim=norm_dims, keepdim=True) + 1e-6)
        y_manual = x / rms_manual * rmsnorm.weight
        
        # æ£€æŸ¥æ•°å€¼ä¸€è‡´æ€§
        assert torch.allclose(y, y_manual, atol=1e-6), "Forward pass mismatch!"
        print("âœ… Forward pass matches manual computation.")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦çœŸçš„è¢«å½’ä¸€åŒ–ï¼ˆRMS â‰ˆ 1ï¼Œå¿½ç•¥ weightï¼‰
        y_unscaled = y / rmsnorm.weight  # ç§»é™¤ weight å½±å“
        rms_out = torch.sqrt(y_unscaled.pow(2).mean(dim=norm_dims, keepdim=True))
        assert torch.allclose(rms_out, torch.ones_like(rms_out), atol=1e-5), "Output RMS not ~1!"
        print("âœ… Output RMS â‰ˆ 1 (after removing weight).")
        
        # æ¢¯åº¦æ£€æŸ¥
        loss = y.sum()
        loss.backward()
        assert x.grad is not None and x.grad.shape == x.shape, "Gradient not computed correctly!"
        if rmsnorm.weight is not None:
            assert rmsnorm.weight.grad is not None, "Weight gradient missing!"
        print("âœ… Gradients computed successfully.")

    print("\nğŸ‰ All tests passed!")

# ----------------------------
# è¿è¡Œæµ‹è¯•
# ----------------------------
if __name__ == "__main__":
    test_rmsnorm()