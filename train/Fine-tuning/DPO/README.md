# DPO(Direct Preference Optimization)

DPO是一种基于人类偏好优化语言方法的新方法，其相比与RLHF更加简单。其不需要设计复杂的损失函数，直接在训练阶段

DPO依赖理论上的偏好模型，如Bradley-Terry模型，来测量奖励函数和经验偏好数据的对齐程度。与传统方法不同，传统方法使用偏好模型来训练奖励模型，然后基于该奖励模型训练策略，DPO直接根据策略定义偏好损失。给定一个关于模型响应的人类偏好数据集，DPO可以使用简单的二元交叉熵目标来优化策略，无需在训练过程中明确学习奖励函数或从策略中采样。