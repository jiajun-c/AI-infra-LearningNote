{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]  # 句子数据集\n",
    "n_steps = 2  # 用前几个单词来预测下一个单词，e.g. 2个\n",
    "n_hidden = 2  # 隐藏层的节点个数，e.g. 2个\n",
    "m = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未去重词表： ['i', 'like', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk']\n",
      "去重词表： ['milk', 'like', 'i', 'hate', 'love', 'dog', 'coffee']\n",
      "单词索引： {'milk': 0, 'like': 1, 'i': 2, 'hate': 3, 'love': 4, 'dog': 5, 'coffee': 6}\n",
      "索引单词： {0: 'milk', 1: 'like', 2: 'i', 3: 'hate', 4: 'love', 5: 'dog', 6: 'coffee'}\n",
      "单词总数： 7\n"
     ]
    }
   ],
   "source": [
    "word_list = \" \".join(sentences).split(\" \")  # 获取所有的单词\n",
    "print(\"未去重词表：\", word_list)\n",
    "word_list = list(set(word_list))  # 去重\n",
    "print(\"去重词表：\", word_list)\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}  # 单词->索引\n",
    "print(\"单词索引：\", word_dict)\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}  # 索引->单词\n",
    "print(\"索引单词：\", number_dict)\n",
    "num_words = len(word_dict)  # 单词总数\n",
    "print(\"单词总数：\", num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.C = nn.Embedding(num_embeddings = num_words, embedding_dim = m)  # 词表\n",
    "        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))  # 隐藏层的偏置\n",
    "        self.H = nn.Parameter(torch.randn(n_steps * m, n_hidden).type(dtype))  # 输入层到隐藏层的权重\n",
    "        self.U = nn.Parameter(torch.randn(n_hidden, num_words).type(dtype))  # 隐藏层到输出层的权重\n",
    "        self.b = nn.Parameter(torch.randn(num_words).type(dtype))  # 输出层的偏置\n",
    "        self.W = nn.Parameter(torch.randn(n_steps * m, num_words).type(dtype))  # 输入层到输出层的权重\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input: [batchsize, n_steps] \n",
    "        x: [batchsize, n_steps*m]\n",
    "        hidden_layer: [batchsize, n_hidden]\n",
    "        output: [batchsize, num_words]\n",
    "        '''\n",
    "        x = self.C(input)  # 获得一个batch的词向量的词表\n",
    "        x = x.view(-1, n_steps * m)\n",
    "        hidden_out = torch.tanh(torch.mm(x, self.H) + self.d)  # 获取隐藏层输出\n",
    "        output = torch.mm(x, self.W) + torch.mm(hidden_out, self.U) + self.b  # 获得输出层输出\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch: tensor([[2, 1],\n",
      "        [2, 4],\n",
      "        [2, 3]])\n",
      "target_batch: tensor([5, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "def make_batch(sentences):\n",
    "  '''\n",
    "  input_batch：一组batch中前n_steps个单词的索引\n",
    "  target_batch：一组batch中每句话待预测单词的索引\n",
    "  '''\n",
    "  input_batch = []\n",
    "  target_batch = []\n",
    "  for sentence in sentences:\n",
    "    word = sentence.split()\n",
    "    input = [word_dict[w] for w in word[:-1]]\n",
    "    target = word_dict[word[-1]]\n",
    "    input_batch.append(input)\n",
    "    target_batch.append(target)\n",
    "  return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "print(\"input_batch:\", input_batch)\n",
    "print(\"target_batch:\", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 Loss:1.122\n",
      "Epoch:200 Loss:0.746\n",
      "Epoch:300 Loss:0.563\n",
      "Epoch:400 Loss:0.474\n",
      "Epoch:500 Loss:0.399\n",
      "Epoch:600 Loss:0.310\n",
      "Epoch:700 Loss:0.221\n",
      "Epoch:800 Loss:0.153\n",
      "Epoch:900 Loss:0.110\n",
      "Epoch:1000 Loss:0.082\n",
      "Epoch:1100 Loss:0.064\n",
      "Epoch:1200 Loss:0.051\n",
      "Epoch:1300 Loss:0.042\n",
      "Epoch:1400 Loss:0.035\n",
      "Epoch:1500 Loss:0.030\n",
      "Epoch:1600 Loss:0.026\n",
      "Epoch:1700 Loss:0.022\n",
      "Epoch:1800 Loss:0.019\n",
      "Epoch:1900 Loss:0.017\n",
      "Epoch:2000 Loss:0.015\n"
     ]
    }
   ],
   "source": [
    "model = NNLM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # 使用cross entropy作为loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)  # 使用Adam作为optimizer\n",
    "\n",
    "for epoch in range(2000):\n",
    "  # 梯度清零\n",
    "  optimizer.zero_grad()\n",
    "  # 计算predication\n",
    "  output = model(input_batch)\n",
    "  # 计算loss\n",
    "  loss = criterion(output, target_batch)\n",
    "  if (epoch + 1) % 100 == 0:\n",
    "    print(\"Epoch:{}\".format(epoch+1), \"Loss:{:.3f}\".format(loss))\n",
    "  # 反向传播\n",
    "  loss.backward()\n",
    "  # 更新权重参数\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: tensor([[5],\n",
      "        [6],\n",
      "        [0]])\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']] ----> ['dog', 'coffee', 'milk']\n"
     ]
    }
   ],
   "source": [
    "pred = model(input_batch).data.max(1, keepdim=True)[1]  # 找出概率最大的下标\n",
    "print(\"Predict:\", pred)\n",
    "print([sentence.split()[:2] for sentence in sentences], \"---->\", [number_dict[n.item()] for n in pred.squeeze()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
